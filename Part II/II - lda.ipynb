{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-based Information Retrieval\n",
    "\n",
    "## Assignment PART II\n",
    "### Using wordembedding\n",
    "We can use the semantic similarity of wordembeddings, such as GloVe and Word2Vec, to obtain better results.\n",
    "In this part of the exercise, we will the addition analogy (similar to Part I of this assignment) to rank the given documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport logging\\nlogger = logging.getLogger()\\nfhandler = logging.FileHandler(filename='part_II_logs.log', mode='a')\\nformatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\\nfhandler.setFormatter(formatter)\\nlogger.addHandler(fhandler)\\nlogger.setLevel(logging.DEBUG)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading modules\n",
    "import os, re\n",
    "import pandas as pd\n",
    "from numpy import dot, sum\n",
    "from gensim import matutils, models, corpora, similarities\n",
    "import gensim\n",
    "\n",
    "# Set up logger that logs (works in jupyter 3!) in console and outputs in file\n",
    "'''\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='part_II_logs.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-9ff3174874ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Note: This will take a lot of memory and can take a while.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Note II: Depending on your RAM, do not load all models at the same time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mw2v_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/GoogleNews-vectors-negative300.bin.gz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#w2v_model.init_sims(replace=True) # Normalize; Trims unneeded model memory = use (much) less RAM.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors)\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m             \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m             \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# throws for invalid file format\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;31m# local files -- both read & write supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;31m# compression, if any, is determined by the filename extension (.gz, .bz2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"s3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0ms3_connection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect_s3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maws_access_key_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccess_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maws_secret_access_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccess_secret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[1;34m(fname, mode)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'.gz'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mgzip\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmake_closing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "source": [
    "# Load Googles' pre-trained Word2Vec vector set\n",
    "# Note: This will take a lot of memory and can take a while.\n",
    "# Note II: Depending on your RAM, do not load all models at the same time\n",
    "w2v_model = models.Word2Vec.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "#w2v_model.init_sims(replace=True) # Normalize; Trims unneeded model memory = use (much) less RAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/glove.6B.50d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-bda4f8a6dba7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# These vectors are stored in a plain text - vector dimensionality 50, 100, 200 and 300\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# only the vectors pre-trained on Wikipedia.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mglove50d_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglove2word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/glove.6B.50d.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-bda4f8a6dba7>\u001b[0m in \u001b[0;36mglove2word2vec\u001b[1;34m(glove_filename)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mnum_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mgensim_first_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"{} {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mmodel_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepend_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgensim_first_line\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-bda4f8a6dba7>\u001b[0m in \u001b[0;36mget_info\u001b[1;34m(glove_filename)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mglove2word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mnum_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglove_filename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# file name contains the number of dimensions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnum_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[1;31m# local files -- both read & write supported\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;31m# compression, if any, is determined by the filename extension (.gz, .bz2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfile_smart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"s3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"s3n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[0ms3_connection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect_s3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maws_access_key_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccess_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maws_secret_access_key\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccess_secret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mfile_smart_open\u001b[1;34m(fname, mode)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmake_closing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/glove.6B.50d.txt'"
     ]
    }
   ],
   "source": [
    "import smart_open\n",
    "import os.path\n",
    "\n",
    "def glove2word2vec(glove_filename):\n",
    "    def get_info(glove_filename): \n",
    "        num_lines = sum(1 for line in smart_open.smart_open(glove_filename))\n",
    "        dims = glove_filename.split('.')[2].split('d')[0] # file name contains the number of dimensions\n",
    "        return num_lines, dims\n",
    "    \n",
    "    def prepend_info(infile, outfile, line): # Function to prepend lines using smart_open\n",
    "        with open(infile, 'r', encoding=\"utf8\") as original: data = original.read()\n",
    "        with open(outfile, 'w', encoding=\"utf8\") as modified: modified.write(line + '\\n' + data)\n",
    "        return outfile\n",
    "    \n",
    "    word2vec_filename = glove_filename[:-3] + \"word2vec.txt\"\n",
    "    if os.path.isfile(word2vec_filename):\n",
    "        model = models.Word2Vec.load_word2vec_format(word2vec_filename)\n",
    "    else:\n",
    "        num_lines, dims = get_info(glove_filename)\n",
    "        gensim_first_line = \"{} {}\".format(num_lines, dims)\n",
    "        model_file = prepend_info(glove_filename, word2vec_filename, gensim_first_line)\n",
    "        model = models.Word2Vec.load_word2vec_format(model_file)\n",
    "    \n",
    "    model.init_sims(replace = True)  # normalize all word vectors\n",
    "    return model\n",
    "\n",
    "# Load GloVes' pre-trained model\n",
    "# These vectors are stored in a plain text - vector dimensionality 50, 100, 200 and 300\n",
    "# only the vectors pre-trained on Wikipedia.\n",
    "glove50d_model = glove2word2vec('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images to wordvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the similarity of wordmodels such as Word2Vec and GloVe to make vectors of each image. These vectors will look like \n",
    ">s = w1 + w2 + ... + wn\n",
    "\n",
    "> With s = the image vector and {w1 .. wn} the words for each image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in a stopword list from\n",
    "# http://www.lextek.com/manuals/onix/stopwords2.html\n",
    "stopwords = []\n",
    "with open('data/stopwordlist.txt', 'r') as f:\n",
    "    lines = ''.join(f.readlines())\n",
    "    stopwords = [x for x in lines.split('\\n')[2:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-12-f7732056e666>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-f7732056e666>\"\u001b[1;36m, line \u001b[1;32m16\u001b[0m\n\u001b[1;33m    print 'word not in model:', word\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "# Translate text to avg vector\n",
    "def sentence_to_vector(model, sentence):\n",
    "    v1 = []\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            v1.append(model[word])\n",
    "        except:\n",
    "            if \"-\" in word: # attempt dash removing or replacing with space\n",
    "                try:\n",
    "                     v1.append(model[word.replace(\"-\", \"\")])\n",
    "                except:\n",
    "                    try:\n",
    "                        v1.append(model[word.split(\"-\")[0]])\n",
    "                        v1.append(model[word.split(\"-\")[1]])\n",
    "                    except:\n",
    "                        print 'word not in model:', word\n",
    "                        continue\n",
    "            else:\n",
    "                print 'word not in model:', word\n",
    "                continue\n",
    "    # return matutils.unitvec(array(v1).mean(axis=0))\n",
    "    return matutils.unitvec(sum(v1,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Clean input because the wordmodels can not contain every possible combination words and signs\n",
    "def clean_input(text, stopwords):\n",
    "    # lowecase and remove linebreaks\n",
    "    text = text.lower().rstrip()\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    # Remove punctuation\n",
    "    text = re.sub('[!@#$:;%&?,_\\.\\'\\`\\\"\\\\\\/\\(\\)\\[\\]]', '', text)\n",
    "    text = re.sub('[\\-]+', '-', text)\n",
    "    # Remove sole numbers, dashes or extra spaces\n",
    "    text = re.sub('[\\s][\\-]+[\\s]', '', text)\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    # British to American English - at this moment still hardcoded due to lack of library\n",
    "    text = text.replace('grey', 'gray')\n",
    "    text = text.replace('colour', 'color')\n",
    "    text = text.replace('tyre', 'tire')\n",
    "    text = text.replace('centre', 'center')\n",
    "    text = text.replace('theatre', 'theater')\n",
    "    text = text.replace('jewellery','jewelry')\n",
    "    text = text.replace('aeroplane', 'plane')\n",
    "    text = text.replace('harbour', 'harbor')\n",
    "    text = text.replace('moustache','mustache')\n",
    "    text = text.replace(' axe', ' hatchet')\n",
    "    text = text.replace('armour', 'armor')\n",
    "    text = text.replace('stylised', 'stylized')\n",
    "    text = text.replace('organise', 'organize')\n",
    "    text = text.replace('plough', 'plow')\n",
    "    text = text.replace('neighbourhood', 'neighborhood')\n",
    "    text = text.replace('vapour', 'vapor')\n",
    "    # some manual fixes of lemmatizing\n",
    "    text = text.replace('watersid ', 'waterside ')\n",
    "    text = text.replace('figur ', 'figure ')\n",
    "    text = text.replace(' graz ', ' graze ')\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Text file parser\n",
    "# Returns a dictionary with imageid - text in lowercase without stopwords or punctuation\n",
    "def text_file_parser(filename, stopwords):\n",
    "    corpus = dict()\n",
    "    #corpus = pd.DataFrame(columns=('id', 'imageid', 'vec'))\n",
    "    with open(filename) as f:\n",
    "        next(f) # skip first line with the headings\n",
    "        for doc in f:\n",
    "            '''\n",
    "            # Normal:\n",
    "            doc_parts = doc.split('\\t')\n",
    "            doc_parts[2] = clean_input(doc_parts[2])\n",
    "            doc_parts.append(sentence_to_vector(model, doc_parts[2]))\n",
    "            corpus[doc_parts[0]] = doc_parts\n",
    "            \n",
    "            # If use of parsed\n",
    "            '''\n",
    "            # Split on spaces\n",
    "            doc_parts = doc.split(\" \", 1)\n",
    "            # If first part is the ID (needed for the queries file)\n",
    "            if(len(doc_parts[0]) < 6):\n",
    "                doc_parts = doc.split(\" \", 2)\n",
    "                doc_parts.pop(0)\n",
    "            # Clean the caption text (remove puctuation etc)\n",
    "            doc_parts[1] = clean_input(doc_parts[1], stopwords)\n",
    "            # add the array (or row) to an array\n",
    "            corpus[len(corpus) + 1] = doc_parts        \n",
    "    # Transform to dataframe\n",
    "    df = pd.DataFrame.from_dict(corpus, orient='index')\n",
    "    df = df.reset_index()\n",
    "    df.columns = ['index', 'img_id', 'caption']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>img_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>XBZPztvt67qkMUdI</td>\n",
       "      <td>man white shirt sit table cut meat plate front...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>PaqtOaYmQmXkqW2i</td>\n",
       "      <td>woman red dress posing hatchet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>IPcFtNL-7EQ6Z0yu</td>\n",
       "      <td>soccer play stand soccer ball front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>IMAD0sq2Fz7HpSgX</td>\n",
       "      <td>white yellow train track</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-gqRDDfPZTGlCfJa</td>\n",
       "      <td>view tall building city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>xsrYb57vl4qiMLDG</td>\n",
       "      <td>hand pick flower vine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>BCjxgJlQ3TD5T8ST</td>\n",
       "      <td>picture army ready sail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>LGxwsl9CtRQ8wW3Y</td>\n",
       "      <td>brick roof house picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>9LtOvyiygFYoxU8S</td>\n",
       "      <td>man clean mess street</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>8usTLD-Wg5EHCShk</td>\n",
       "      <td>group kid play playground accompany adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>kH59MJp3nWyFfFB2</td>\n",
       "      <td>woman black shirt jean pant picture red carpet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>tluPF-CA6dN6LACF</td>\n",
       "      <td>boat sail body water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>HKXYBXXObkt_yi7s</td>\n",
       "      <td>crowd people stadium watch baseball game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>mBJjuuB0ukfKmnRH</td>\n",
       "      <td>men drag large snake tail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>cqpTaVCZZe5OpuIk</td>\n",
       "      <td>group people street beach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Anry0qU5NFes6Twh</td>\n",
       "      <td>woman sit bottom bed neat hotel room</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>ByjuNEqsLcbUk5OH</td>\n",
       "      <td>man wear glove mask destroy wall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>5dBiwoDEpY6gWRek</td>\n",
       "      <td>brick entrance building tree background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>ZFdHuWXCk662UDAW</td>\n",
       "      <td>woman bright red lipstick sing large microphone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index            img_id                                            caption\n",
       "0       1  XBZPztvt67qkMUdI  man white shirt sit table cut meat plate front...\n",
       "1       2  PaqtOaYmQmXkqW2i                     woman red dress posing hatchet\n",
       "2       3  IPcFtNL-7EQ6Z0yu                soccer play stand soccer ball front\n",
       "3       4  IMAD0sq2Fz7HpSgX                           white yellow train track\n",
       "4       5  -gqRDDfPZTGlCfJa                            view tall building city\n",
       "5       6  xsrYb57vl4qiMLDG                              hand pick flower vine\n",
       "6       7  BCjxgJlQ3TD5T8ST                            picture army ready sail\n",
       "7       8  LGxwsl9CtRQ8wW3Y                           brick roof house picture\n",
       "8       9  9LtOvyiygFYoxU8S                              man clean mess street\n",
       "9      10  8usTLD-Wg5EHCShk          group kid play playground accompany adult\n",
       "10     11  kH59MJp3nWyFfFB2     woman black shirt jean pant picture red carpet\n",
       "11     12  tluPF-CA6dN6LACF                               boat sail body water\n",
       "12     13  HKXYBXXObkt_yi7s           crowd people stadium watch baseball game\n",
       "13     14  mBJjuuB0ukfKmnRH                          men drag large snake tail\n",
       "14     15  cqpTaVCZZe5OpuIk                          group people street beach\n",
       "15     16  Anry0qU5NFes6Twh               woman sit bottom bed neat hotel room\n",
       "16     17  ByjuNEqsLcbUk5OH                   man wear glove mask destroy wall\n",
       "17     18  5dBiwoDEpY6gWRek            brick entrance building tree background\n",
       "18     19  ZFdHuWXCk662UDAW    woman bright red lipstick sing large microphone"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# images file to docs dict\n",
    "print('Parsing documents')\n",
    "training_docs = text_file_parser('data/target_collection_parsed.txt', stopwords)\n",
    "docs = []\n",
    "for text in training_docs['caption']:\n",
    "    docs.append(text.split())\n",
    "dictionary = corpora.Dictionary(docs)\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]   \n",
    "# generate LDA model\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=10)\n",
    "queries = text_file_parser('data/queries_val_parsed20.txt', stopwords)\n",
    "\n",
    "# Preview\n",
    "queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to calculate the similarity between 2 documents\n",
    "def similarity(v1, v2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two documents.\n",
    "    Example:\n",
    "      >>> trained_model.similarity('doc1', 'doc2')\n",
    "      0.73723527\n",
    "      >>> trained_model.similarity('doc2', 'doc2')\n",
    "      1.0\n",
    "    \"\"\"\n",
    "    #return dot(matutils.unitvec(v1), matutils.unitvec(v1))\n",
    "    return dot(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying\n"
     ]
    }
   ],
   "source": [
    "# Calculate similarity\n",
    "#training_docs[\"sim\"] = 0.0\n",
    "queries['recall'] = 0.0\n",
    "queries['precision'] = 0.0\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "length = len(corpus)\n",
    "index = similarities.Similarity('./tmp/tst',corpus, num_features=length)\n",
    "print('querying')\n",
    "for i,r in queries.iterrows():\n",
    "    \n",
    "    # Calculate similarity per vector document\n",
    "    vec_bow = dictionary.doc2bow(r['caption'].split())\n",
    "    vec_lda = lda[vec_bow]\n",
    "    sims = index[vec_lda]\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    sims = [sim[0] for sim in sims[:1000]]\n",
    "    \n",
    "    # Recall\n",
    "    total_to_find = float(len(training_docs[training_docs.img_id == r['img_id']]))\n",
    "    correct = 0\n",
    "    for sim in sims:\n",
    "        if training_docs[training_docs.index == sim].values[0][1] == r['img_id']:\n",
    "            correct += 1\n",
    "    \n",
    "    amount_found = correct\n",
    "    recall = float(amount_found/total_to_find)\n",
    "    queries.set_value(i, 'recall', recall)\n",
    "    \n",
    "    # Precision\n",
    "    precision = float(amount_found/float(1000))\n",
    "    queries.set_value(i, 'precision', precision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying results:\n",
      "--\n",
      "AVG recall 0.04727684596105649\n",
      "AVG precision: 0.0004210526315789474\n",
      "--\n",
      "    index            img_id  \\\n",
      "0       1  XBZPztvt67qkMUdI   \n",
      "1       2  PaqtOaYmQmXkqW2i   \n",
      "2       3  IPcFtNL-7EQ6Z0yu   \n",
      "3       4  IMAD0sq2Fz7HpSgX   \n",
      "4       5  -gqRDDfPZTGlCfJa   \n",
      "5       6  xsrYb57vl4qiMLDG   \n",
      "6       7  BCjxgJlQ3TD5T8ST   \n",
      "7       8  LGxwsl9CtRQ8wW3Y   \n",
      "8       9  9LtOvyiygFYoxU8S   \n",
      "9      10  8usTLD-Wg5EHCShk   \n",
      "10     11  kH59MJp3nWyFfFB2   \n",
      "11     12  tluPF-CA6dN6LACF   \n",
      "12     13  HKXYBXXObkt_yi7s   \n",
      "13     14  mBJjuuB0ukfKmnRH   \n",
      "14     15  cqpTaVCZZe5OpuIk   \n",
      "15     16  Anry0qU5NFes6Twh   \n",
      "16     17  ByjuNEqsLcbUk5OH   \n",
      "17     18  5dBiwoDEpY6gWRek   \n",
      "18     19  ZFdHuWXCk662UDAW   \n",
      "\n",
      "                                              caption    recall  precision  \n",
      "0   man white shirt sit table cut meat plate front...  0.000000      0.000  \n",
      "1                      woman red dress posing hatchet  0.000000      0.000  \n",
      "2                 soccer play stand soccer ball front  0.000000      0.000  \n",
      "3                            white yellow train track  0.000000      0.000  \n",
      "4                             view tall building city  0.047619      0.002  \n",
      "5                               hand pick flower vine  0.000000      0.000  \n",
      "6                             picture army ready sail  0.200000      0.001  \n",
      "7                            brick roof house picture  0.166667      0.001  \n",
      "8                               man clean mess street  0.000000      0.000  \n",
      "9           group kid play playground accompany adult  0.000000      0.000  \n",
      "10     woman black shirt jean pant picture red carpet  0.000000      0.000  \n",
      "11                               boat sail body water  0.000000      0.000  \n",
      "12           crowd people stadium watch baseball game  0.000000      0.000  \n",
      "13                          men drag large snake tail  0.333333      0.002  \n",
      "14                          group people street beach  0.000000      0.000  \n",
      "15               woman sit bottom bed neat hotel room  0.125000      0.001  \n",
      "16                   man wear glove mask destroy wall  0.000000      0.000  \n",
      "17            brick entrance building tree background  0.025641      0.001  \n",
      "18    woman bright red lipstick sing large microphone  0.000000      0.000  \n"
     ]
    }
   ],
   "source": [
    "print(\"Displaying results:\")\n",
    "\n",
    "# Remove irrelevant columns\n",
    "\n",
    "# Save results\n",
    "queries.to_csv(path_or_buf='data/results_part2_wordembedding.csv')\n",
    "\n",
    "print(\"--\")\n",
    "print(\"AVG recall\", queries.recall.mean())\n",
    "print(\"AVG precision:\",  queries.precision.mean())\n",
    "print(\"--\")\n",
    "\n",
    "# print results\n",
    "print(queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results from calculations on server\n",
    "For lowering the workload on our computers, we let a server do the calculations and write the results to a csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in results file\n",
    "df = pd.DataFrame.from_csv('results/results.csv')\n",
    "\n",
    "# averages\n",
    "print \"--\"\n",
    "print \"AVG recall\", df.recall.mean()\n",
    "print \"AVG precision:\",  df.precision.mean()\n",
    "print \"--\"\n",
    "\n",
    "# Preview dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort by recall\n",
    "df = df.sort_values(by=['recall'], ascending=[1])\n",
    "\n",
    "# Show and save chart\n",
    "qx = df.plot(x='recall', y='precision')\n",
    "fig = qx.get_figure()\n",
    "fig.savefig('results/part2_embedding_precision-recall.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
