We compare the use of different word embeddings (GloVe and Word2Vec) towards different analogy arithmetic models\cite{marekreiblog}. To test these, we used the questions provided by Google\cite{word2vecquestionwords}.
\newline
\newline
We coded our solution in Python (version 2.7) using Jupyter Notebook. All experiments were run on a notebook with 8GB RAM memory, Intel(R) CORE\textsuperscript{TM}i7-3610QM CPU, SSD hard disk and a Windows 7 Operating System.
\newline
\newline
We use freely available pre-trained vector models for GloVe \cite{glovedata} and Word2Ve \cite{word2vecdata}. The GloVe models are almost the same format as the Word2Vec models, the only difference is that Word2Vec had a header with the dimensions. Once these headers are added, the GloVe model can also run with the default Gensim operations for Word2Vec.
\newline
\newline
To run the different analogy arithmetic models (see next paragraph), we introduce a separate function in our code for each model. These functions then use the Gensim library for Python \cite{gensim} to calculate the different vector distances.
\newline
\newline
\textbf{An analogy arithmetic function} is used to calculate how close the input vector is to another vector in order to choose the most similar result. To calculate the angle between high dimensional vectors, Mikolov et al. \cite{mikolov} proposed using cosine similarity as explained in equation \ref{cosinesimilarity}. 
\begin{equation}
\begin{split}
\label{cosinesimilarity}
similarity(v1, v2) &= cos(\theta) \\&= \dfrac{v1 \cdot v2}{||v1|| \cdot ||v2||}  \\&= \dfrac{\sum_{i=1}^{n}v1_i v2_i}{\sqrt{\sum_{i=1}^{n}v1_i^2} \sqrt{\sum_{i=1}^{n} v2_i^2}}
\end{split}
\end{equation}

\leavevmode
\newline
We consider two analogy arithmetic models. The first is named the \textbf{Addition method} and is simply maximizing the cosine similarity between the result $d_w$ and the vector equation $c-a+b$, where the variables $a, b and c$ have the same meaning as mentioned in the warm-up exercise. This method actually looks for words similar to c and b, and not to a. In equation \ref{additionmethod} we introduce $V$ as the whole vocabulary and $d'_w$ as a chosen element from $V$.

\begin{equation}
\begin{split}
\label{multiplicationmethod}
v &= c-a+b\\
d_w &= argmax_{d'_w \in V}(similarity(d'_w, v) )
\end{split}
\end{equation}
\newline
The other method was referred to as the \textbf{Multiplication method} \cite{leviandgoldberg}. Because the Addition method risks being dominated by one large term, therefore the authors proposed that instead of adding the similarities, they could be multiplied as described in equation \ref{multiplicationmethod}.

\begin{equation}
\begin{split}
\label{multiplicationmethod}
d_w &= argmax_{d'_w \in V}(\dfrac{similarity(d'_w, c) similarity(d'_w, b)}{similarity(d'_w, a) + \epsilon} )
\end{split}
\end{equation}
\newline
\textbf{Two different recall numbers} are generated if a word does not occur within the pre-trained vector model: One where the \textbf{missing word is considered a failure} for the analogy and one where the \textbf{missing word is just ignored}. We could also have done a third option, were we use the nearest word in the vector model instead of the missing word. However, we did not do this because this would take a lot more computing power and (as seen in the results below) we're not sure whether it would have made a difference.