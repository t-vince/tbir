This part of the assignment is based on the annual Scalable Concept Image Annotation Challenge, organized by ImageCLEF \cite{imageclef}. The dataset (given by ImageCLEF), contains sentence descriptions aligned to actual images. In this part of the assignment, we bring our previous tests into a real-life retrieval scenario: retrieve images by their descriptions using textual queries. Each query has a corresponding image ID, which can be used to verify the correctness of the model. The goal is to match queries with image descriptions as good as possible, using precision and recall as evaluation measures.
\newline
\newline
First we implemented a simple language model to retrieve the images, this was based on a unigram implementation \cite{languagemodelsforinformationretrieval}. Afterwards we extended it with word embedding, inspired by part I of this assignment. For the final method, we chose to implement Latent Dirichlet Allocation (LDA) with dirichlet priors. We have read that this performs well with twitter data, where the document size seems similar to our image descriptions. And this is a method that has been mentioned several times in class which sparked our interest. 
