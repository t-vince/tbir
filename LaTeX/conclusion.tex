In retrospect to the methods, we have learned that the older models, such as the Simple Language Models performs very well while taking way less computational power. Our favorite method was Word2Vec, but it takes a lot of RAM memory. In fact, the used models took up all our RAM which resulted in extra long running times because some things had to be written on the hard disk.
And finally, hyper-parameters make a big difference, something that we noticed most using LDA.
\newline
\newline
In retrospect of the assignment, we may need to start sooner and dwell less on fine-tuning less important parts of the assignment. And when using methods like Word2Vec, a cloud computer or online server can save a lot of time.
\newline
\newline
Overall, it was an interesting to work with the algorithms and see empirical results instead of just the theoretical results. We have experienced at first-hand what difficulties occur during the experiments (e.g. how important it is that all words are in a similar form).