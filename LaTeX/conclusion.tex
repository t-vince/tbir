In retrospect to the methods, we have learned that the older models, such as the Simple Language Models performs very well while taking way less computational power. Our favorite method was Word2Vec (mostly because it gave the best results), but it takes a lot of RAM memory. In fact, the used models took up all our RAM which resulted in extra long running times because some things had to be written on the hard disk.
And finally, hyper-parameters (as used in LDA) should give better results, it was quite challenging (and fun) to analyze why the results were not as expected.
\newline
\newline
In retrospect of the assignment, we need to divide our time better, ie. prioritizing certain aspects instead of focussing on eg fine-tuning. And when using methods like Word2Vec, a cloud computer or online server can save a lot of time.
\newline
\newline
Overall, it was an interesting to work with the algorithms and see empirical results instead of just the theoretical results. We have experienced at first-hand what difficulties occur during the experiments (e.g. how important it is that all words are in a similar form).