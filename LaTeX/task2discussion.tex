\subsubsection{Unigram Language Model}


\subsubsection{Wordembedding}
Because the location of the word in the model gives semantic similarity, words with similar meanings are given similar vectors. This results in a more accurate result than the Unigram Language Model, as we can see in the results.
Because of the limited size of the documents, a lot of similar words are still found. If the descriptions would be longer, a more accurate result could be found and the overall accuracy would increase. But with the current length of the queries and the target collection, we think we got acceptable results.

To calculate similarity, we sum the different vectors. The main problem with this is that longer descriptions get a more accurate result if they contain the right words.  [I think]
A more intelligent way to do this would be [I have no idea]


\subsubsection{LSI}
